"""
Native Anthropic messages endpoint handler.
"""
import json
import logging
import time
import uuid
from fastapi import APIRouter, HTTPException, Request
from fastapi.responses import StreamingResponse

from anthropic import (
    AnthropicMessageRequest,
    sanitize_anthropic_request,
    inject_claude_code_system_message,
    add_prompt_caching,
    make_anthropic_request,
    stream_anthropic_response,
)
from anthropic.thinking_keywords import process_thinking_keywords
from anthropic.beta_headers import build_beta_headers
from proxy.thinking_storage import inject_thinking_blocks
from models.resolution import resolve_model_metadata
from models.reasoning import REASONING_BUDGET_MAP
from oauth import OAuthManager
import settings
from stream_debug import maybe_create_stream_tracer
from ..logging_utils import log_request
from utils.usage_recorder import record_request_usage

logger = logging.getLogger(__name__)
router = APIRouter()
oauth_manager = OAuthManager()


@router.post("/v1/messages")
async def anthropic_messages(request: AnthropicMessageRequest, raw_request: Request):
    """Native Anthropic messages endpoint"""
    request_id = str(uuid.uuid4())[:8]
    start_time = time.time()

    # Capture raw request headers
    headers_dict = dict(raw_request.headers)

    logger.info(f"[{request_id}] ===== NEW ANTHROPIC MESSAGES REQUEST =====")
    log_request(request_id, request.model_dump(), "/v1/messages", headers_dict)

    # Get valid access token with automatic refresh
    access_token = await oauth_manager.get_valid_token_async()
    if not access_token:
        logger.error(f"[{request_id}] No valid token available")
        raise HTTPException(
            status_code=401,
            detail={"error": {"message": "OAuth expired; please authenticate using the CLI"}}
        )

    # Prepare Anthropic request (pass through client parameters directly)
    anthropic_request = request.model_dump()

    # Model resolution logic to detect reasoning variants and update request model
    original_model = anthropic_request.get("model")
    reasoning_level = None
    use_1m_context = False

    if original_model:
        resolved_model, reasoning_level, use_1m_context = resolve_model_metadata(original_model)
        anthropic_request["model"] = resolved_model

        if resolved_model != original_model or reasoning_level or use_1m_context:
            logger.debug(f"[{request_id}] Model resolution: {original_model} -> {resolved_model}, "
                        f"reasoning={reasoning_level}, 1m_context={use_1m_context}")

    # Process thinking keywords in messages (detect, strip, and get config)
    messages = anthropic_request.get("messages", [])
    processed_messages, thinking_config = process_thinking_keywords(messages)

    if thinking_config:
        anthropic_request["messages"] = processed_messages
        # Only set thinking if not already configured
        if not anthropic_request.get("thinking"):
            anthropic_request["thinking"] = thinking_config
            logger.info(f"[{request_id}] Injected thinking config from keyword: {thinking_config}")
        else:
            # Update budget if keyword specifies higher budget
            existing_budget = anthropic_request["thinking"].get("budget_tokens", 0)
            keyword_budget = thinking_config.get("budget_tokens", 0)
            if keyword_budget > existing_budget:
                anthropic_request["thinking"]["budget_tokens"] = keyword_budget
                logger.info(f"[{request_id}] Updated thinking budget from {existing_budget} to {keyword_budget}")

    # Ensure max_tokens is sufficient if thinking is enabled
    thinking = anthropic_request.get("thinking")
    if thinking and thinking.get("type") == "enabled":
        thinking_budget = thinking.get("budget_tokens", 16000)
        min_response_tokens = 1024
        required_total = thinking_budget + min_response_tokens
        if anthropic_request["max_tokens"] < required_total:
            anthropic_request["max_tokens"] = required_total
            logger.debug(f"[{request_id}] Increased max_tokens to {required_total} (thinking: {thinking_budget} + response: {min_response_tokens})")

        # Inject stored thinking blocks from previous responses
        anthropic_request["messages"] = inject_thinking_blocks(anthropic_request["messages"])
        logger.debug(f"[{request_id}] Injected stored thinking blocks if available")

    # Sanitize request for Anthropic API constraints
    anthropic_request = sanitize_anthropic_request(anthropic_request)

    # Inject Claude Code system message to bypass authentication detection
    anthropic_request = inject_claude_code_system_message(anthropic_request)

    # Add cache_control to message content blocks for optimal caching
    anthropic_request = add_prompt_caching(anthropic_request, ttl=settings.CACHE_TTL)

    # Enforce thinking budget for reasoning models
    if reasoning_level and reasoning_level in REASONING_BUDGET_MAP:
        # Check if request already has thinking parameter
        existing_thinking = anthropic_request.get("thinking")
        required_budget = REASONING_BUDGET_MAP[reasoning_level]

        if not existing_thinking or existing_thinking.get("type") != "enabled":
            # Add thinking parameter with appropriate budget
            anthropic_request["thinking"] = {
                "type": "enabled",
                "budget_tokens": required_budget
            }
            logger.debug(f"[{request_id}] Added thinking parameter with budget {required_budget} for reasoning level '{reasoning_level}'")
        else:
            # Ensure existing thinking budget meets minimum requirement
            existing_budget = existing_thinking.get("budget_tokens", 0)
            if existing_budget < required_budget:
                anthropic_request["thinking"]["budget_tokens"] = required_budget
                logger.debug(f"[{request_id}] Updated thinking budget from {existing_budget} to {required_budget} for reasoning level '{reasoning_level}'")

    # Extract client beta headers and build consolidated beta header value
    client_beta_headers = headers_dict.get("anthropic-beta")
    beta_header_value = build_beta_headers(
        anthropic_request,
        client_beta_headers=client_beta_headers,
        request_id=request_id,
        for_streaming=request.stream,
        reasoning_level=reasoning_level,
        use_1m_context=use_1m_context,
    )

    logger.debug(f"[{request_id}] FINAL ANTHROPIC REQUEST HEADERS: authorization=Bearer *****, anthropic-beta={beta_header_value}, User-Agent=Claude-Code/1.0.0")
    logger.debug(f"[{request_id}] SYSTEM MESSAGE STRUCTURE: {json.dumps(anthropic_request.get('system', []), indent=2)}")
    logger.debug(f"[{request_id}] FULL REQUEST COMPARISON - Our request structure:")
    logger.debug(f"[{request_id}] - model: {anthropic_request.get('model')}")
    system = anthropic_request.get('system')
    if system:
        logger.debug(f"[{request_id}] - system: {type(system)} with {len(system)} elements")
    else:
        logger.debug(f"[{request_id}] - system: None")
    logger.debug(f"[{request_id}] - messages: {len(anthropic_request.get('messages', []))} messages")
    logger.debug(f"[{request_id}] - stream: {anthropic_request.get('stream')}")
    logger.debug(f"[{request_id}] - temperature: {anthropic_request.get('temperature')}")
    logger.debug(f"[{request_id}] FULL REQUEST BODY: {json.dumps(anthropic_request, indent=2)}")

    try:
        if request.stream:
            # Handle streaming response
            logger.debug(f"[{request_id}] Initiating streaming request")
            tracer = maybe_create_stream_tracer(
                enabled=settings.STREAM_TRACE_ENABLED,
                request_id=request_id,
                route="anthropic-messages",
                base_dir=settings.STREAM_TRACE_DIR,
                max_bytes=settings.STREAM_TRACE_MAX_BYTES,
            )

            # Variables to capture usage from streaming response
            stream_usage = {
                "input_tokens": 0,
                "output_tokens": 0,
                "cache_read_input_tokens": 0,
                "cache_creation_input_tokens": 0,
            }
            api_key_id = getattr(raw_request.state, 'api_key_id', None)
            stream_model = anthropic_request.get("model", "")

            async def raw_stream():
                nonlocal stream_usage
                try:
                    async for chunk in stream_anthropic_response(
                        request_id,
                        anthropic_request,
                        access_token,
                        beta_header_value,
                        tracer=tracer,
                    ):
                        # Parse SSE chunk to extract usage from message_delta event
                        if chunk and b'"usage"' in chunk:
                            try:
                                # Parse SSE format: data: {...}
                                for line in chunk.decode('utf-8', errors='ignore').split('\n'):
                                    if line.startswith('data: '):
                                        data = json.loads(line[6:])
                                        if data.get("type") == "message_delta":
                                            usage = data.get("usage", {})
                                            if usage.get("output_tokens"):
                                                stream_usage["output_tokens"] = usage.get("output_tokens", 0)
                                        elif data.get("type") == "message_start":
                                            message = data.get("message", {})
                                            usage = message.get("usage", {})
                                            stream_usage["input_tokens"] = usage.get("input_tokens", 0)
                                            stream_usage["cache_read_input_tokens"] = usage.get("cache_read_input_tokens", 0)
                                            stream_usage["cache_creation_input_tokens"] = usage.get("cache_creation_input_tokens", 0)
                            except (json.JSONDecodeError, UnicodeDecodeError):
                                pass
                        yield chunk
                finally:
                    if tracer:
                        tracer.close()
                    # Record usage after stream completes
                    if stream_usage["input_tokens"] > 0 or stream_usage["output_tokens"] > 0:
                        record_request_usage(
                            key_id=api_key_id,
                            model=stream_model,
                            input_tokens=stream_usage["input_tokens"],
                            output_tokens=stream_usage["output_tokens"],
                            cache_read_tokens=stream_usage["cache_read_input_tokens"],
                            cache_creation_tokens=stream_usage["cache_creation_input_tokens"],
                            request_id=request_id
                        )

            return StreamingResponse(
                raw_stream(),
                media_type="text/event-stream",
                headers={"Cache-Control": "no-cache", "Connection": "keep-alive"}
            )
        else:
            # Handle non-streaming response
            logger.debug(f"[{request_id}] Making non-streaming request")
            response = await make_anthropic_request(anthropic_request, access_token, beta_header_value)

            elapsed_ms = int((time.time() - start_time) * 1000)
            logger.info(f"[{request_id}] Anthropic request completed in {elapsed_ms}ms status={response.status_code}")

            if response.status_code != 200:
                try:
                    error_json = response.json()
                except Exception:
                    # If not JSON, return raw text
                    error_json = {"error": {"type": "api_error", "message": response.text}}

                logger.error(f"[{request_id}] Anthropic API error {response.status_code}: {json.dumps(error_json)}")

                # FastAPI will automatically set the status code and return this as JSON
                raise HTTPException(status_code=response.status_code, detail=error_json)

            # Return Anthropic response as-is (native format)
            anthropic_response = response.json()
            final_elapsed_ms = int((time.time() - start_time) * 1000)

            # Log usage information for debugging
            usage_info = anthropic_response.get("usage", {})
            input_tokens = usage_info.get("input_tokens", 0)
            output_tokens = usage_info.get("output_tokens", 0)
            total_tokens = input_tokens + output_tokens
            logger.debug(f"[{request_id}] [DEBUG] Response usage: input={input_tokens}, output={output_tokens}, total={total_tokens}")

            # Record usage for tracking
            record_request_usage(
                key_id=getattr(raw_request.state, 'api_key_id', None),
                model=anthropic_request.get("model", ""),
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                cache_read_tokens=usage_info.get("cache_read_input_tokens", 0),
                cache_creation_tokens=usage_info.get("cache_creation_input_tokens", 0),
                request_id=request_id
            )

            logger.info(f"[{request_id}] ===== ANTHROPIC MESSAGES FINISHED ===== Total time: {final_elapsed_ms}ms")
            return anthropic_response

    except HTTPException:
        final_elapsed_ms = int((time.time() - start_time) * 1000)
        logger.error(f"[{request_id}] ===== ANTHROPIC MESSAGES FAILED ===== Total time: {final_elapsed_ms}ms")
        raise
    except Exception as e:
        final_elapsed_ms = int((time.time() - start_time) * 1000)
        logger.error(f"[{request_id}] Request failed after {final_elapsed_ms}ms: {e}")
        raise HTTPException(status_code=500, detail={"error": {"message": str(e)}})

